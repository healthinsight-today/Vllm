# vLLM Qwen2.5 32B Deployment Requirements
# Core dependencies for production deployment

# Primary inference engine
vllm>=0.9.0

# PyTorch with CUDA support (installed via deploy script)
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# Model and tokenizer support
transformers>=4.36.0
tokenizers>=0.15.0

# HTTP client libraries for testing
requests>=2.31.0
httpx>=0.25.0
aiohttp>=3.9.0

# Utility libraries
numpy>=1.24.0
packaging>=21.0

# Optional: Monitoring and debugging
psutil>=5.9.0
GPUtil>=1.4.0

# Development and testing
pytest>=7.0.0
pytest-asyncio>=0.21.0